{
    "docs": [
        {
            "location": "/", 
            "text": "Diary of an Engineer\n\n\nHere you will find all the documentation and links to code, boards, and designs for projects i'm working on. \n\n\n\n\nmoboNet\n\n\nSome home automation work I've done over the years. \n\n\n\n\nAndroid Continuum\n\n\nProject to create a handheld \"Synthesizer\" on a Nexus 7 tablet.\n\n\n\n\nFM Demodulator\n\n\nSharing of the creation of a hardcoded frequency demodulator for listening to VHF on an SDR in GNU Radio. \n\n\n\n\nROCKETS\n\n\nWho doesn't like rockets?\n\n\n\n\n\n\nNote\n\n\nI'm constantly messing with this site, projects may be missing related code and files but will be updated.", 
            "title": "Home"
        }, 
        {
            "location": "/#diary-of-an-engineer", 
            "text": "Here you will find all the documentation and links to code, boards, and designs for projects i'm working on.", 
            "title": "Diary of an Engineer"
        }, 
        {
            "location": "/#mobonet", 
            "text": "Some home automation work I've done over the years.", 
            "title": "moboNet"
        }, 
        {
            "location": "/#android-continuum", 
            "text": "Project to create a handheld \"Synthesizer\" on a Nexus 7 tablet.", 
            "title": "Android Continuum"
        }, 
        {
            "location": "/#fm-demodulator", 
            "text": "Sharing of the creation of a hardcoded frequency demodulator for listening to VHF on an SDR in GNU Radio.", 
            "title": "FM Demodulator"
        }, 
        {
            "location": "/#rockets", 
            "text": "Who doesn't like rockets?    Note  I'm constantly messing with this site, projects may be missing related code and files but will be updated.", 
            "title": "ROCKETS"
        }, 
        {
            "location": "/mobonet/", 
            "text": "moboNet: A Home Automation Experiment\n\n\nDescription\n\n\nThis goal of this project was to create an affordable and expandable home network system and along the way learn Java, C#, Matlab, OrCAD, and PSpice. Also you may be wondering, why the heck did i call this 'mobonet'? Well long story short, as a 6 year old i thought 'mrmoebob' was a hilarious monikor, which eventually became 'mobo'. Alternative reason: I'm not so good at naming things. \n\n\n\n\nVideo\n\n\n\n\n\n\n\n\nDiagram\n\n\n\n\n\nComponents\n\n\nClient\n\n\nThe clients consist of any device with the means of sending commands to the server module. This means Android application, Desktop application in addition to the door knocker module which sends notifications over the 315MHz network.\n\n\nServer\n\n\nThe server module is an arduino uno with an Ethernet shield which transfers data from LAN to a 418MHz network. I designed a relatively simple communication protocol for the RF links to provide reliable communication from server to light module. \n\n\nLight Modules\n\n\nThe light module consists of RGB LEDs which receives commands via the 418MHz network. The might module is what sits under the cube in the video and will eventually be equipped to be screwed into a standard light bulb socket.\n\n\n\n\nControl\n\n\nAndroid Application\n\n\n\n\n\n\n\n\n\n\nThis is the Android application I wrote to control the light modules from my phone. With this application i can control all aspects of the module in addition to the 4 channels of the RF AC relays. To communicate with the sever it must be connected to the wifi network that the server is on however I am working on a secure way to control the lights from anywhere so that i can control systems when I'm away.\n\n\nDesktop Application\n\n\n\n\nWritten in C# in visual studio, the desktop application operates in the same way as the Android application, by sending UDP packets to the server on the network. \n\n\n\n\nLight Module\n\n\nDesign\n\n\n\n\n\nSchematic\n\n\n\n\n\n\nPCB\n\n\n\n\n\nAudio Mode and Signal Analysis\n\n\nDescription\n\n\nThis mode is a part of the light module which responds to music, here i will go over how the input signal is generated and processed.\n\n\nCircuit\n\n\nThere are five main components to the Light module. \n\n\n\n\nControl System\n\n\nPower Supply\n\n\nLEDs and current driving circuit\n\n\nCommunication\n\n\nAnalog audio processing circuit\n\n\n\n\nThe control system i use is an ATMega238 which ties all the components together. The power supply for this circuit i use a 9V supply into a 5V regulator for logic. The LEDs i use are two 3W RGB LEDs in series. Each diode is 1W and there are 3 individual LEDs in each package. As for communication i use the RX module of a 418MHz RF link to receive messages from the server. The analog circuit consists of a BJT preamplifier in a common emitter configuration which provides around 40dB of amplification. This is not enough by itself however so i then cascade that into an LM386 amplifier in a configuration providing another 20dB of amplification which puts the voltage into a range easily readable for the ADC on the microcontroller.\n\n\nFrequency Response\n\n\n\n\n\nHere are the stages of the analog frequency response. It is generated from the circuit model in PSpice by performing an AC frequency sweep from 1Hz to 1MHz.\n\n\nDigital Sampling and Filtering\n\n\nAfter the signal is amplified and filtered, it is then sampled at 20Khz. Further processing is then done through digital filtering. I am using a 30th order FIR filter. This method doesn't provide continuous digital signal but rather a burst of 31 samples every time the audio processing loop is entered. I have found that with the limited processing speed of the ATMEGA328, it is best to sample in this method rather than is interrupt based. Currently there are only two channels, 0-150Hz and 1KHz-10KHz. I hope to push this to more resolution in the future so i can design a music algorithm with more complexity while still staying within a 60Hz loop frequency. When the computations begin to take longer than ~20mS there becomes noticeable jitter. I may yet be able to compensate for this if i need to when adding more channels.", 
            "title": "moboNet"
        }, 
        {
            "location": "/mobonet/#mobonet-a-home-automation-experiment", 
            "text": "", 
            "title": "moboNet: A Home Automation Experiment"
        }, 
        {
            "location": "/mobonet/#description", 
            "text": "This goal of this project was to create an affordable and expandable home network system and along the way learn Java, C#, Matlab, OrCAD, and PSpice. Also you may be wondering, why the heck did i call this 'mobonet'? Well long story short, as a 6 year old i thought 'mrmoebob' was a hilarious monikor, which eventually became 'mobo'. Alternative reason: I'm not so good at naming things.", 
            "title": "Description"
        }, 
        {
            "location": "/mobonet/#video", 
            "text": "", 
            "title": "Video"
        }, 
        {
            "location": "/mobonet/#diagram", 
            "text": "", 
            "title": "Diagram"
        }, 
        {
            "location": "/mobonet/#components", 
            "text": "", 
            "title": "Components"
        }, 
        {
            "location": "/mobonet/#client", 
            "text": "The clients consist of any device with the means of sending commands to the server module. This means Android application, Desktop application in addition to the door knocker module which sends notifications over the 315MHz network.", 
            "title": "Client"
        }, 
        {
            "location": "/mobonet/#server", 
            "text": "The server module is an arduino uno with an Ethernet shield which transfers data from LAN to a 418MHz network. I designed a relatively simple communication protocol for the RF links to provide reliable communication from server to light module.", 
            "title": "Server"
        }, 
        {
            "location": "/mobonet/#light-modules", 
            "text": "The light module consists of RGB LEDs which receives commands via the 418MHz network. The might module is what sits under the cube in the video and will eventually be equipped to be screwed into a standard light bulb socket.", 
            "title": "Light Modules"
        }, 
        {
            "location": "/mobonet/#control", 
            "text": "", 
            "title": "Control"
        }, 
        {
            "location": "/mobonet/#android-application", 
            "text": "This is the Android application I wrote to control the light modules from my phone. With this application i can control all aspects of the module in addition to the 4 channels of the RF AC relays. To communicate with the sever it must be connected to the wifi network that the server is on however I am working on a secure way to control the lights from anywhere so that i can control systems when I'm away.", 
            "title": "Android Application"
        }, 
        {
            "location": "/mobonet/#desktop-application", 
            "text": "Written in C# in visual studio, the desktop application operates in the same way as the Android application, by sending UDP packets to the server on the network.", 
            "title": "Desktop Application"
        }, 
        {
            "location": "/mobonet/#light-module", 
            "text": "", 
            "title": "Light Module"
        }, 
        {
            "location": "/mobonet/#design", 
            "text": "", 
            "title": "Design"
        }, 
        {
            "location": "/mobonet/#schematic", 
            "text": "", 
            "title": "Schematic"
        }, 
        {
            "location": "/mobonet/#pcb", 
            "text": "", 
            "title": "PCB"
        }, 
        {
            "location": "/mobonet/#audio-mode-and-signal-analysis", 
            "text": "", 
            "title": "Audio Mode and Signal Analysis"
        }, 
        {
            "location": "/mobonet/#description_1", 
            "text": "This mode is a part of the light module which responds to music, here i will go over how the input signal is generated and processed.", 
            "title": "Description"
        }, 
        {
            "location": "/mobonet/#circuit", 
            "text": "There are five main components to the Light module.    Control System  Power Supply  LEDs and current driving circuit  Communication  Analog audio processing circuit   The control system i use is an ATMega238 which ties all the components together. The power supply for this circuit i use a 9V supply into a 5V regulator for logic. The LEDs i use are two 3W RGB LEDs in series. Each diode is 1W and there are 3 individual LEDs in each package. As for communication i use the RX module of a 418MHz RF link to receive messages from the server. The analog circuit consists of a BJT preamplifier in a common emitter configuration which provides around 40dB of amplification. This is not enough by itself however so i then cascade that into an LM386 amplifier in a configuration providing another 20dB of amplification which puts the voltage into a range easily readable for the ADC on the microcontroller.", 
            "title": "Circuit"
        }, 
        {
            "location": "/mobonet/#frequency-response", 
            "text": "Here are the stages of the analog frequency response. It is generated from the circuit model in PSpice by performing an AC frequency sweep from 1Hz to 1MHz.", 
            "title": "Frequency Response"
        }, 
        {
            "location": "/mobonet/#digital-sampling-and-filtering", 
            "text": "After the signal is amplified and filtered, it is then sampled at 20Khz. Further processing is then done through digital filtering. I am using a 30th order FIR filter. This method doesn't provide continuous digital signal but rather a burst of 31 samples every time the audio processing loop is entered. I have found that with the limited processing speed of the ATMEGA328, it is best to sample in this method rather than is interrupt based. Currently there are only two channels, 0-150Hz and 1KHz-10KHz. I hope to push this to more resolution in the future so i can design a music algorithm with more complexity while still staying within a 60Hz loop frequency. When the computations begin to take longer than ~20mS there becomes noticeable jitter. I may yet be able to compensate for this if i need to when adding more channels.", 
            "title": "Digital Sampling and Filtering"
        }, 
        {
            "location": "/android_continuum/", 
            "text": "Introduction\n\n\nIn this project we plan on implementing an Android synthesizer. Using this app will allow for manipulation of a variety of basic waveforms. [1] The user will utilize all three axes, similar to Professor Lippold Haken\u2019s Fingerboard Continuum. While not as complex as the Continuum, we hope to implement signal processing ideas to design a small scale continuous synthesizer.\n\n\nThe core of the project revolves around wavetable synthesis. The wavetable contains various waveforms. There are three different sounds that the Android synthesizer has implemented: sine wave, square wave, sawtooth wave. The synthesizer also uses sample-based synthesis to generate sound based on pre-recorded sound samples.  The user also has the option to record a sound and added it to the table of waveforms. Various characteristics of the waveforms can be altered in real-time. A tablet allows the user to use all three axes to control different characteristics of the output sound. The x-axis controls the frequency of the sound. The y-axis control the strength of the Karplus-Strong algorithm that is applied to the sound. [2] Karplus-Strong synthesis utilizes delay and filtering to create a reverberation effect. The z-axis modifies the overall amplitude of the sound. All the waveforms and samples are affected by all three axes.\n\n\nResearch\n\n\nThe article [2] by Karplus and Strong goes into detail about their algorithm for synthesizing musical instruments. The basic method is wavetable synthesis. The main principle in wavetable synthesis is that there is a bank of waveforms, which can be repeated to create a sustained note. This method is dull because it produces purely periodic tones. There is no flavor to the sound, such as timbre which all real instruments posses. In the Karplus-Strong synthesis, the wavetable becomes a delay line. The simplest modification is to average successive samples. This effectively produces a decay in the sound. In this manner, when a sound contains high frequencies, these are averaged out first. This method acts as a filter, slowly reducing high frequencies first and leaving low frequencies for last. Another modification that can be made is to introduce randomness in the delay line. Randomness aids in simulating different instances of the same note on the same instrument. In reality, no note played twice sounds the same even on the same instrument. This factor is applied to digital sound synthesis with randomness. The introduction of randomness also can add effects such as a glissando, tie, or slur. While the Karplus-Strong algorithm can be applied to reproduce string instruments quite well, it can also synthesize drum timbres. This is accomplished by changing the probability that values in the delay line are modified. The details and math are documented in article [2]. One of the final methods Karplus and Strong discuss is decay stretching for higher frequencies. Since high pitches have shorter periods, they do not fill up the delay line. This problem is solved by implementing a stretch factor. They offer tips for implementing their algorithm by using a decreasing-counter or a circular buffer.\n\n\nWe used the Lent algorithm discussed in his paper [3] to implement pitch shifting. The Lent algorithm can be broken up into three sections: pitch tracker, compressor/expander, and pitch shifter. The pitch tracker can be crude and inaccurate. It only needs to calculate a rough estimate of the fundamental pitch. In his paper, Lent uses zero crossing after filtering out the high frequencies. The compressor/expander compensates for the timing differences when the pitch shifting is applied. Lent discusses how the simplest method for pitch shifting is resampling at higher or lower rates to change the frequency. He offers a better solution. Still using one period of the sample, window the sound. This way the ends of the samples are zero. The key in the Lent algorithm is changing the period of the sample. To achieve a lower sound, the period needs to increase. This is accomplished by zero-padding the end of the original sample. For higher pitches, the period is shortened by overlapping and adding the sound together.\n\n\nDescription\n\n\n\n\nThe preprocessing block contains the procedure for the user generated sample-based synthesis. As seen in Figure 2, after the tablet receives audio input from the microphone, the sample is analyzed to identify the fundamental frequency. This is essential for placing the sample correctly on the interface of the synthesizer. Originally we intended to use harmonic-product spectrum, but this proved difficult since not all sounds have the same number of harmonics. Instead, we chose to use zero-crossing to identify the period and thus the frequency. While this method is not entirely reliable, it is sufficient for placing the sample. \n\n\n\n\nThe waveform module in Figure 3 contains the necessary components for wavetable synthesis. It receives the X and Z axes data from the screen to control the frequency and amplitude, respectively. After the frequency and amplitude have been specified, they are fed into the wavetable. The wavetable houses the different waveforms. [3] We implemented the Lent algorithm for pitch shifting the waveforms. In the Lent algorithm, pitch shifting is achieved by varying the period of the sound, thus altering the pitch. Lowering the pitch is achieved by increasing the period through zero-padding. Higher pitches require a shorter period. In this case, waveforms are overlapped and added to preserve the original sound and still shift the pitch. The waveform module also receives the recorded sound and corresponding pitch and stores it in the wavetable.\n\n\n[3] According to the Lent Algorithm, the ratio between the fundamental frequency and the target frequency is used to determine the change in period.\n\n\n\n\n \\frac{T_{fundamental}}{T_{target}} = \\frac{f_{target}}{f_{fundamental}}\n\n\n\n\n\n\nKarplus-Strong is the final element of the project. It is controlled by the Y axis, and applies the Karplus-Strong effect to the sound if is toggled on. The equation for Karplus-Strong is shown below, where coefficient a is determined by the Y axis and filters the output sound. \n\n\n\n\nx(n) = x(n) + a[\\frac{x(n-1) + x(n-2)}{2}] \\quad where \\quad a < 1\n\n\n\n\nThe value of coefficient a increases at the bottom of the screen, effectively creating a longer sustain. As the user slides up the Y axis, the filtering strengthens and reduces the sustain effect. \n\n\n\n\nResults\n\n\nThe primary method for testing the effectiveness and correctness of our code was that of auditory comparison to reference material. For the case of sample based synthesis, reference waveforms of sin, square, and sawtooth are played to which the tablet implementation is compared. There were not any problems of note relating to creating the wave table synthesis. \n\n\nFor testing the sample based synthesis, analogous algorithms were constructed in MATLAB, which enables the use of visual debugging via printing the waveforms as they are generated. Once the MATLAB implementation worked correctly, it was then used as a reference sound for the Android implementation. This was also the method used for testing the user input sample based synthesis. \nWhile testing there were of course a myriad of bugs and troubles most of which were specific to the Android platform and Java and not implementing the signal processing examples. \n\n\nIn Figure 5 below, the graphical user interface (GUI) of the synthesizer is shown. The screen is separated with white lines to differentiate notes. The letter on the top and number of the bottom corresponds to the note name and frequency located in the center of the section. The scale from left to right is not linear to account for whole and half steps between notes. For example, in order to keep the sections the same size, the scale between the notes E and F (half step) need to be different than the one between notes C and D (whole step). In the top right corner are a few options. The \u2013 and + give the user control of the octave they wish to play in. The option for \u2018Toggle Play\u2019 turns the synthesizer on and off. \u2018String Mode\u2019 applies Karplus-Strong. \u2018Record\u2019 records a short sound sample to be used for the user generated sample-based synthesis. \n\n\n\n\nFig. 5 Continuum GUI\n\n\nFigure 6 shows the menu of the GUI. This menu is essentially the wavetable. The user has the option to choose which waveform they wish to use.\n\n\n\nFig. 6 Continuum GUI with menu open\n\n\nFuture Work\n\n\nA number of features of the Android Continuum could be improved in future developments. The quality of the prerecorded samples could improve. Currently they vaguely resemble the actual instruments, but using higher quality waveforms will help with the synthesis.\nAnother improvement would be with the speed of the touch events. Touch events are not triggered often enough, which prevents the synthesizer to be truly continuous. At slow speeds, the synthesizer appears to shift pitch continuously. If the user moves too fast, the pitch will skip and become discontinuous.\nLastly, additional features and effects can be added to the synthesizer. There is almost no limit to the number of features that can be added. A future developer can add more instruments as prerecorded sounds. Additional effects can be added as well, such as distortion or a harmonizer.\n\n\nSoftware\n\n\nWe used an Android tablet, Google Nexus 7, to implement the synthesizer. The screen provides position and pressure for multiple finger touches. The Nexus 7 has a sampling rate of 48 kHz and a microphone for audio input. The current design has the capabilities of 10 distinct touches. It is recommended to limit touch events to 4 or 5 because quality decreases significantly with increased touch events.\n\n\nThere are four main files which make up the core of the project. Three of these are classes which perform the different syntheses and the last is the main application function. All of the coding was done in Java for the Android platform. The target operating system is Kitkat (4.4.1) although it has limited support on versions as low as Ice Cream Sandwich (4.0) where multi-touch doesn\u2019t work. The development IDE used is Android Studio, and most XML for the UI was generated through the available GUI builder. \n\n\nThe main file of the four is called MainActivity.java and contains mostly control software to make the application work in addition to implementation of Karplus-Strong string synthesis. The three main parts of the file are the touch event handlers, output thread, and worker thread. The touch event handlers are what is called when the system registers a touch event on the activity. This is the area where positions and pressures are updated in addition to a few triggers for the Karplus-Strong mode. Once a touch event has happened, the computation of the output sound is done in the worker thread. The purpose of the worker thread is to create the next frame of output data while the current frame is being pushed to the output buffer. The worker thread is where the different synthesis classes are constructed and executed. The output threads\u2019 only task is to grab data when it is available and writes it to the output audio device. The reason that the output and worker threads are separate is because writing the data to the output is a blocking task. The same operations could, also be implemented using the AsyncTask class in Android. \n\n\nThe other three files of interest are the synthesis classes. The first is the wave table synthesis file which is a class called WaveSynth.java. It has three modes which return appropriately sized buffers of sin, sawtooth, or square wave. The second, which is a class called SampleSynth.java, is the class for using sample based synthesis. It currently has five pre-programmed instruments which need to be declared upon construction. The last is the user input sample based synthesis class which is called userSynth.java. The user synth is essentially the same as the sample synth except that It gets its samples from recording microphone input. \n\n\nFor more information about the specific methods and operation of the files and classes mentioned above, the source for these files is available in the appendices at the end of the report.\n\n\nCitations\n\n\n[1] L. Haken, \u201cHaken Audio Continuum Fingerboard,\u201d [Online]. Available: http://www.hakenaudio.com/Continuum/ [Accessed: 11 March 2014].\n\n\n[2] K. Karplus and A. Strong, \u201cDigital Synthesis of Plucked-String and Drum Timbres,\u201d Computer Music Journal, vol. 7, no. 2, Summer 1983, pp 43-55. [Online]. Available: JSTOR, http://www.jstor.org/stable/3680062 [Accessed: 11 March 2014]. \n\n\n[3] K. Lent, \u201cAn Efficient Method for Pitch Shifting Digitally Sampled Sounds,\u201d Computer Music Journal, vol. 13, no. 4, Winter 1989, pp. 65-71. [Online]. Available: JSTOR, http://www.jstor.org/stable/3679554 [Accessed: 13 April 2014].\n\n\n[4] Patricio de la Cuadra, PITCH DETECTION METHODS REVIEW, [Online]. Available: https://ccrma.stanford.edu/~pdelac/154/m154paper.htm [Accessed: 11 March 2014].", 
            "title": "Android Continuum"
        }, 
        {
            "location": "/android_continuum/#introduction", 
            "text": "In this project we plan on implementing an Android synthesizer. Using this app will allow for manipulation of a variety of basic waveforms. [1] The user will utilize all three axes, similar to Professor Lippold Haken\u2019s Fingerboard Continuum. While not as complex as the Continuum, we hope to implement signal processing ideas to design a small scale continuous synthesizer.  The core of the project revolves around wavetable synthesis. The wavetable contains various waveforms. There are three different sounds that the Android synthesizer has implemented: sine wave, square wave, sawtooth wave. The synthesizer also uses sample-based synthesis to generate sound based on pre-recorded sound samples.  The user also has the option to record a sound and added it to the table of waveforms. Various characteristics of the waveforms can be altered in real-time. A tablet allows the user to use all three axes to control different characteristics of the output sound. The x-axis controls the frequency of the sound. The y-axis control the strength of the Karplus-Strong algorithm that is applied to the sound. [2] Karplus-Strong synthesis utilizes delay and filtering to create a reverberation effect. The z-axis modifies the overall amplitude of the sound. All the waveforms and samples are affected by all three axes.", 
            "title": "Introduction"
        }, 
        {
            "location": "/android_continuum/#research", 
            "text": "The article [2] by Karplus and Strong goes into detail about their algorithm for synthesizing musical instruments. The basic method is wavetable synthesis. The main principle in wavetable synthesis is that there is a bank of waveforms, which can be repeated to create a sustained note. This method is dull because it produces purely periodic tones. There is no flavor to the sound, such as timbre which all real instruments posses. In the Karplus-Strong synthesis, the wavetable becomes a delay line. The simplest modification is to average successive samples. This effectively produces a decay in the sound. In this manner, when a sound contains high frequencies, these are averaged out first. This method acts as a filter, slowly reducing high frequencies first and leaving low frequencies for last. Another modification that can be made is to introduce randomness in the delay line. Randomness aids in simulating different instances of the same note on the same instrument. In reality, no note played twice sounds the same even on the same instrument. This factor is applied to digital sound synthesis with randomness. The introduction of randomness also can add effects such as a glissando, tie, or slur. While the Karplus-Strong algorithm can be applied to reproduce string instruments quite well, it can also synthesize drum timbres. This is accomplished by changing the probability that values in the delay line are modified. The details and math are documented in article [2]. One of the final methods Karplus and Strong discuss is decay stretching for higher frequencies. Since high pitches have shorter periods, they do not fill up the delay line. This problem is solved by implementing a stretch factor. They offer tips for implementing their algorithm by using a decreasing-counter or a circular buffer.  We used the Lent algorithm discussed in his paper [3] to implement pitch shifting. The Lent algorithm can be broken up into three sections: pitch tracker, compressor/expander, and pitch shifter. The pitch tracker can be crude and inaccurate. It only needs to calculate a rough estimate of the fundamental pitch. In his paper, Lent uses zero crossing after filtering out the high frequencies. The compressor/expander compensates for the timing differences when the pitch shifting is applied. Lent discusses how the simplest method for pitch shifting is resampling at higher or lower rates to change the frequency. He offers a better solution. Still using one period of the sample, window the sound. This way the ends of the samples are zero. The key in the Lent algorithm is changing the period of the sample. To achieve a lower sound, the period needs to increase. This is accomplished by zero-padding the end of the original sample. For higher pitches, the period is shortened by overlapping and adding the sound together.", 
            "title": "Research"
        }, 
        {
            "location": "/android_continuum/#description", 
            "text": "The preprocessing block contains the procedure for the user generated sample-based synthesis. As seen in Figure 2, after the tablet receives audio input from the microphone, the sample is analyzed to identify the fundamental frequency. This is essential for placing the sample correctly on the interface of the synthesizer. Originally we intended to use harmonic-product spectrum, but this proved difficult since not all sounds have the same number of harmonics. Instead, we chose to use zero-crossing to identify the period and thus the frequency. While this method is not entirely reliable, it is sufficient for placing the sample.    The waveform module in Figure 3 contains the necessary components for wavetable synthesis. It receives the X and Z axes data from the screen to control the frequency and amplitude, respectively. After the frequency and amplitude have been specified, they are fed into the wavetable. The wavetable houses the different waveforms. [3] We implemented the Lent algorithm for pitch shifting the waveforms. In the Lent algorithm, pitch shifting is achieved by varying the period of the sound, thus altering the pitch. Lowering the pitch is achieved by increasing the period through zero-padding. Higher pitches require a shorter period. In this case, waveforms are overlapped and added to preserve the original sound and still shift the pitch. The waveform module also receives the recorded sound and corresponding pitch and stores it in the wavetable.  [3] According to the Lent Algorithm, the ratio between the fundamental frequency and the target frequency is used to determine the change in period.    \\frac{T_{fundamental}}{T_{target}} = \\frac{f_{target}}{f_{fundamental}}    Karplus-Strong is the final element of the project. It is controlled by the Y axis, and applies the Karplus-Strong effect to the sound if is toggled on. The equation for Karplus-Strong is shown below, where coefficient a is determined by the Y axis and filters the output sound.    x(n) = x(n) + a[\\frac{x(n-1) + x(n-2)}{2}] \\quad where \\quad a < 1   The value of coefficient a increases at the bottom of the screen, effectively creating a longer sustain. As the user slides up the Y axis, the filtering strengthens and reduces the sustain effect.", 
            "title": "Description"
        }, 
        {
            "location": "/android_continuum/#results", 
            "text": "The primary method for testing the effectiveness and correctness of our code was that of auditory comparison to reference material. For the case of sample based synthesis, reference waveforms of sin, square, and sawtooth are played to which the tablet implementation is compared. There were not any problems of note relating to creating the wave table synthesis.   For testing the sample based synthesis, analogous algorithms were constructed in MATLAB, which enables the use of visual debugging via printing the waveforms as they are generated. Once the MATLAB implementation worked correctly, it was then used as a reference sound for the Android implementation. This was also the method used for testing the user input sample based synthesis. \nWhile testing there were of course a myriad of bugs and troubles most of which were specific to the Android platform and Java and not implementing the signal processing examples.   In Figure 5 below, the graphical user interface (GUI) of the synthesizer is shown. The screen is separated with white lines to differentiate notes. The letter on the top and number of the bottom corresponds to the note name and frequency located in the center of the section. The scale from left to right is not linear to account for whole and half steps between notes. For example, in order to keep the sections the same size, the scale between the notes E and F (half step) need to be different than the one between notes C and D (whole step). In the top right corner are a few options. The \u2013 and + give the user control of the octave they wish to play in. The option for \u2018Toggle Play\u2019 turns the synthesizer on and off. \u2018String Mode\u2019 applies Karplus-Strong. \u2018Record\u2019 records a short sound sample to be used for the user generated sample-based synthesis.    Fig. 5 Continuum GUI  Figure 6 shows the menu of the GUI. This menu is essentially the wavetable. The user has the option to choose which waveform they wish to use.  \nFig. 6 Continuum GUI with menu open", 
            "title": "Results"
        }, 
        {
            "location": "/android_continuum/#future-work", 
            "text": "A number of features of the Android Continuum could be improved in future developments. The quality of the prerecorded samples could improve. Currently they vaguely resemble the actual instruments, but using higher quality waveforms will help with the synthesis.\nAnother improvement would be with the speed of the touch events. Touch events are not triggered often enough, which prevents the synthesizer to be truly continuous. At slow speeds, the synthesizer appears to shift pitch continuously. If the user moves too fast, the pitch will skip and become discontinuous.\nLastly, additional features and effects can be added to the synthesizer. There is almost no limit to the number of features that can be added. A future developer can add more instruments as prerecorded sounds. Additional effects can be added as well, such as distortion or a harmonizer.", 
            "title": "Future Work"
        }, 
        {
            "location": "/android_continuum/#software", 
            "text": "We used an Android tablet, Google Nexus 7, to implement the synthesizer. The screen provides position and pressure for multiple finger touches. The Nexus 7 has a sampling rate of 48 kHz and a microphone for audio input. The current design has the capabilities of 10 distinct touches. It is recommended to limit touch events to 4 or 5 because quality decreases significantly with increased touch events.  There are four main files which make up the core of the project. Three of these are classes which perform the different syntheses and the last is the main application function. All of the coding was done in Java for the Android platform. The target operating system is Kitkat (4.4.1) although it has limited support on versions as low as Ice Cream Sandwich (4.0) where multi-touch doesn\u2019t work. The development IDE used is Android Studio, and most XML for the UI was generated through the available GUI builder.   The main file of the four is called MainActivity.java and contains mostly control software to make the application work in addition to implementation of Karplus-Strong string synthesis. The three main parts of the file are the touch event handlers, output thread, and worker thread. The touch event handlers are what is called when the system registers a touch event on the activity. This is the area where positions and pressures are updated in addition to a few triggers for the Karplus-Strong mode. Once a touch event has happened, the computation of the output sound is done in the worker thread. The purpose of the worker thread is to create the next frame of output data while the current frame is being pushed to the output buffer. The worker thread is where the different synthesis classes are constructed and executed. The output threads\u2019 only task is to grab data when it is available and writes it to the output audio device. The reason that the output and worker threads are separate is because writing the data to the output is a blocking task. The same operations could, also be implemented using the AsyncTask class in Android.   The other three files of interest are the synthesis classes. The first is the wave table synthesis file which is a class called WaveSynth.java. It has three modes which return appropriately sized buffers of sin, sawtooth, or square wave. The second, which is a class called SampleSynth.java, is the class for using sample based synthesis. It currently has five pre-programmed instruments which need to be declared upon construction. The last is the user input sample based synthesis class which is called userSynth.java. The user synth is essentially the same as the sample synth except that It gets its samples from recording microphone input.   For more information about the specific methods and operation of the files and classes mentioned above, the source for these files is available in the appendices at the end of the report.", 
            "title": "Software"
        }, 
        {
            "location": "/android_continuum/#citations", 
            "text": "[1] L. Haken, \u201cHaken Audio Continuum Fingerboard,\u201d [Online]. Available: http://www.hakenaudio.com/Continuum/ [Accessed: 11 March 2014].  [2] K. Karplus and A. Strong, \u201cDigital Synthesis of Plucked-String and Drum Timbres,\u201d Computer Music Journal, vol. 7, no. 2, Summer 1983, pp 43-55. [Online]. Available: JSTOR, http://www.jstor.org/stable/3680062 [Accessed: 11 March 2014].   [3] K. Lent, \u201cAn Efficient Method for Pitch Shifting Digitally Sampled Sounds,\u201d Computer Music Journal, vol. 13, no. 4, Winter 1989, pp. 65-71. [Online]. Available: JSTOR, http://www.jstor.org/stable/3679554 [Accessed: 13 April 2014].  [4] Patricio de la Cuadra, PITCH DETECTION METHODS REVIEW, [Online]. Available: https://ccrma.stanford.edu/~pdelac/154/m154paper.htm [Accessed: 11 March 2014].", 
            "title": "Citations"
        }, 
        {
            "location": "/fm_demod/", 
            "text": "FM Demodulator in GNU Radio\n\n\nDescription\n\n\nI was looking around for a reference GNURadio example which implemented the WBFM manually, but to my surprise I couldn't find one so I made one and thought I'd share. \nIn this GNURadio project file I demonstrate one way you can demodulate FM signal content using only the math and filter blocks. The end result is of course is implemented within the WBFM demodulation block, although i am not sure if they use the same method. \n\n\nBackground\n\n\nFrequency Modulation\n is a method of encoding a signal, such as an audio sample, to the instaneous frequency of sinusoid which has a some center/carrier frequency Fc The below image from Wikipedia does a good job of demonstrating what this looks like in the time domain and compares to \nAM Modulation\n. \n\n\n\n\nThe mathematical expression for a general input signal waveform I(t) with a maximum frequency deviation scalar of kf and center frequency fc is defined by:\n\n\n\n\n Y(t) = A cos(2\\pi f_ct + k_f\\int_{-\\infty}^{t}I(t')dt') \n\n\n\n\nTraditional methods of frequency demodulation can involve differentiation and envelope detection, but since we have directy I/Q samples from the RTL-SDR we can use a more elegant method which only requires basic elements. When we have two data streams of In-phase I(t) and Quadrature Q(t) data, the amplitude and phase components can be retreived as:\n\n\n\n\n A(t) = 2 \\sqrt{I(t)^2 + Q(t)^2} \n\n\n \\theta(t) = -tan^{-1}(\\frac{Q(t)}{I(t)}) \n\n\n\n\nIn the case of our frequency modulated signal of interest Y(t) we are interested in how the phase/angle changes over time. Differentiating the theta(t) with respect to time we have:\n\n\n\n\n (1) \\quad \\quad  \\Delta \\omega(t) = \\frac{d\\theta}{dt} = \\frac{-1}{I^2+Q^2}(I \\frac{dQ}{dt} - Q \\frac{dI}{dt}) \n\n\n\n\nAbove is the continuous time representation of the instantaneous freuency component of the I/Q stream. Since we have descrete samples from our transciever approximate the elementwise derivative of dQ/dt as:\n\n\n\n\n \\frac{dQ}{dt} \\Rightarrow \\frac{Q[n] - Q[n-1]}{\\Delta t} \n\n\n\n\nAllowing us to re-write the descrete version of equation 1 as:\n\n\n\n\n \\require{cancel} \n\n\n\n\n\n\n \\Delta \\omega[n] = -\\frac{I[n]\\frac{Q[n] - Q[n-1]}{\\Delta t} - Q[n]\\frac{I[n] - I[n-1]}{\\Delta t} }{I[n]^2 + Q[n]^2} \n\n\n\n\n\n\n \\Delta \\omega[n] = \\frac{-\\frac{1}{\\Delta t}(I[n](Q[n] - Q[n-1]) - Q[n](I[n] - I[n-1]))}{I[n]^2 + Q[n]^2} \n\n\n\n\n\n\n \\Delta \\omega[n] = \\frac{-\\frac{1}{\\Delta t}(\\cancel{I[n]Q[n]} - I[n]Q[n-1] - \\cancel{Q[n]I[n]} - Q[n]I[n-1])}{I[n]^2 + Q[n]^2} \n\n\n\n\n\n\n \\Delta \\omega[n] = \\frac{\\frac{1}{\\Delta t}(I[n]Q[n-1] + Q[n]I[n-1])}{I[n]^2 + Q[n]^2} \n\n\n\n\n\n\n (2) \\quad \\quad \\Delta \\omega[n] = \\frac{I[n]Q[n-1]-Q[n]I[n-1]}{\\Delta t (I[n]^2 + Q[n]^2)} \n\n\n\n\nIdeally the denominator (which is the magnitude of the I/Q vector) should be a constant, but in the real world this is not always the case. In the image of a FM broadcast below we can see the the scheme provides an amplitude varying about 3 dB. \n\n\n\n\nGNURadio\n\n\nIn the GNURadio implementation below, I use an RTL-SDR as the reciever and use equation 2 from above to demodulate wide band FM broadcasts. GNURadio companion project file can be downloaded \nHere\n.\n\n\n\n\nThe spectrum allocation inside the FM broadcast contains a few main sections. In the image below we can see the baseband signal content from 30 Hz to 15030 Hz is the Left + Right audio information to allow comptability with mono receivers. In the current implemntation of this demodulator we are listening to this L+R mono signal. \n\n\n \n\n\nIn the baseband spectrum plot below, we can see from 0 to around 20 kHz is our L+R spectrum and the 19 kHz pilot frequency clearly.\n\n\n\n\nTo implement stereo audio by utilizing the L-R after mixing down to baseband, the reconstruction is as follows:\n\n\n\n\n Audio_{mono} = Audio_{left} + Audio_{right} \n\n\n\n\n\n\n Audio_{sub} = Audio_{left} - Audio_{right} \n\n\n\n\n\n\n (3) \\quad \\quad Audio_{left} = \\frac{Audio_{mono}+ Audio_{sub}}{2} \n\n\n\n\n\n\n (4) \\quad \\quad Audio_{left} = \\frac{Audio_{mono}- Audio_{sub}}{2} \n\n\n\n\nBelow is a capture of the full GNURadio Gui.\n\n\n\n\nReferences\n\n\n[1] S. Franke, \u201cWireless Communication Systems\u201d University of Illinois at Urbana-Champaign.\n\n\n[2] GNURadio, \"GNU Radio Manual and C++ API Reference\" \nhttp://gnuradio.org/doc/doxygen/\n.", 
            "title": "FM Demodulator"
        }, 
        {
            "location": "/fm_demod/#fm-demodulator-in-gnu-radio", 
            "text": "", 
            "title": "FM Demodulator in GNU Radio"
        }, 
        {
            "location": "/fm_demod/#description", 
            "text": "I was looking around for a reference GNURadio example which implemented the WBFM manually, but to my surprise I couldn't find one so I made one and thought I'd share. \nIn this GNURadio project file I demonstrate one way you can demodulate FM signal content using only the math and filter blocks. The end result is of course is implemented within the WBFM demodulation block, although i am not sure if they use the same method.", 
            "title": "Description"
        }, 
        {
            "location": "/fm_demod/#background", 
            "text": "Frequency Modulation  is a method of encoding a signal, such as an audio sample, to the instaneous frequency of sinusoid which has a some center/carrier frequency Fc The below image from Wikipedia does a good job of demonstrating what this looks like in the time domain and compares to  AM Modulation .    The mathematical expression for a general input signal waveform I(t) with a maximum frequency deviation scalar of kf and center frequency fc is defined by:    Y(t) = A cos(2\\pi f_ct + k_f\\int_{-\\infty}^{t}I(t')dt')    Traditional methods of frequency demodulation can involve differentiation and envelope detection, but since we have directy I/Q samples from the RTL-SDR we can use a more elegant method which only requires basic elements. When we have two data streams of In-phase I(t) and Quadrature Q(t) data, the amplitude and phase components can be retreived as:    A(t) = 2 \\sqrt{I(t)^2 + Q(t)^2}    \\theta(t) = -tan^{-1}(\\frac{Q(t)}{I(t)})    In the case of our frequency modulated signal of interest Y(t) we are interested in how the phase/angle changes over time. Differentiating the theta(t) with respect to time we have:    (1) \\quad \\quad  \\Delta \\omega(t) = \\frac{d\\theta}{dt} = \\frac{-1}{I^2+Q^2}(I \\frac{dQ}{dt} - Q \\frac{dI}{dt})    Above is the continuous time representation of the instantaneous freuency component of the I/Q stream. Since we have descrete samples from our transciever approximate the elementwise derivative of dQ/dt as:    \\frac{dQ}{dt} \\Rightarrow \\frac{Q[n] - Q[n-1]}{\\Delta t}    Allowing us to re-write the descrete version of equation 1 as:    \\require{cancel}      \\Delta \\omega[n] = -\\frac{I[n]\\frac{Q[n] - Q[n-1]}{\\Delta t} - Q[n]\\frac{I[n] - I[n-1]}{\\Delta t} }{I[n]^2 + Q[n]^2}      \\Delta \\omega[n] = \\frac{-\\frac{1}{\\Delta t}(I[n](Q[n] - Q[n-1]) - Q[n](I[n] - I[n-1]))}{I[n]^2 + Q[n]^2}      \\Delta \\omega[n] = \\frac{-\\frac{1}{\\Delta t}(\\cancel{I[n]Q[n]} - I[n]Q[n-1] - \\cancel{Q[n]I[n]} - Q[n]I[n-1])}{I[n]^2 + Q[n]^2}      \\Delta \\omega[n] = \\frac{\\frac{1}{\\Delta t}(I[n]Q[n-1] + Q[n]I[n-1])}{I[n]^2 + Q[n]^2}      (2) \\quad \\quad \\Delta \\omega[n] = \\frac{I[n]Q[n-1]-Q[n]I[n-1]}{\\Delta t (I[n]^2 + Q[n]^2)}    Ideally the denominator (which is the magnitude of the I/Q vector) should be a constant, but in the real world this is not always the case. In the image of a FM broadcast below we can see the the scheme provides an amplitude varying about 3 dB.", 
            "title": "Background"
        }, 
        {
            "location": "/fm_demod/#gnuradio", 
            "text": "In the GNURadio implementation below, I use an RTL-SDR as the reciever and use equation 2 from above to demodulate wide band FM broadcasts. GNURadio companion project file can be downloaded  Here .   The spectrum allocation inside the FM broadcast contains a few main sections. In the image below we can see the baseband signal content from 30 Hz to 15030 Hz is the Left + Right audio information to allow comptability with mono receivers. In the current implemntation of this demodulator we are listening to this L+R mono signal.      In the baseband spectrum plot below, we can see from 0 to around 20 kHz is our L+R spectrum and the 19 kHz pilot frequency clearly.   To implement stereo audio by utilizing the L-R after mixing down to baseband, the reconstruction is as follows:    Audio_{mono} = Audio_{left} + Audio_{right}      Audio_{sub} = Audio_{left} - Audio_{right}      (3) \\quad \\quad Audio_{left} = \\frac{Audio_{mono}+ Audio_{sub}}{2}      (4) \\quad \\quad Audio_{left} = \\frac{Audio_{mono}- Audio_{sub}}{2}    Below is a capture of the full GNURadio Gui.", 
            "title": "GNURadio"
        }, 
        {
            "location": "/fm_demod/#references", 
            "text": "[1] S. Franke, \u201cWireless Communication Systems\u201d University of Illinois at Urbana-Champaign.  [2] GNURadio, \"GNU Radio Manual and C++ API Reference\"  http://gnuradio.org/doc/doxygen/ .", 
            "title": "References"
        }, 
        {
            "location": "/rockets/", 
            "text": "ROCKETS\n\n\nBackground\n\n\nFor the last few years, I've attended a summer BBQ event hosted by the company my girlfriend works for in eastern Pennsylvania. My personal favorite part of this event is the one involving controlled explosions (of course) in which everyone participates in assembling a rocket to fire off. The rockets range from the toy-sized, to the massive almost 6' in length. \n\n\nThis year, I was given A very simple choice of which off the shelf rocket and motor to use. Do I use a \nD6\n engine or a \nC12\n engine with my \nNike-X-7259\n? I mean the choice was simple but not having much experience with hobby rockets and of course when you spend the many of your days modeling systems for engineering purposes, the answer can't be \nthat\n easy...\n\n\nSo I decided to start working on a very simple model of rocket dynamics to try to determine which of these engines will give the best performance. I needed a way to validate my intuition which told me that the larger motor crammed into the smaller rocket body (which it was not setup for) would be the way to go. The result of this pondering is the beginnings of a modeling tool in MATLAB to help me determine the overall trajectory for a given configuration. \n\n\nThe simulation takes into account: Hull Weight, Hull Dimensions, Motor Thrust, Motor Burn Times, Motor Static Weight, Motor Fuel Weight, Motor Thrust. I'm beginning to work on integrating drag and other non-idealities which will probably happen sometime before next years event. Below is a simple comparison of the two configurations. I adjusted the displacement angle slightly off normal so that we can actually see how they might behave more easily. \n\n\n\n\n\nSee MATLAB section below for source\n\n\n\nWell that convinces me enough to go with the D6, showing that even with the higher static weight and shorter burn time, the D6 should provide a higher maximum altitude reached. The only other tricky bit was ensuring that the rocket maintained aerodynamic stability with that added weight in the back (since this rocket was designed to use the C12). To account for how that added clay might impact the results, I added 0.01 kg to the static weight in the Motor1 scenario. \n\n\nAfter a custom engine fitting and excellent paint job by my talented other half, we have a rocket! (Mine is the far left)\n\n\n\n\n\nOn at launch time it had already gotten pretty dark so I've pointed out which one is mine in the image below. It's also the last to leave the platform... So heavy...\n\n\n\n\n\nAnyways, here is the launch. Unfortunately because it was so dark we lost track of it pretty much immediately, but it was found in the end. I'm hoping to eventually compare the launch in greater detail with the simulation but first must add at least some very basic wind effects, since a parachute deploys on descent. Another feature for next time!\n\n\n\n\n\nMATLAB\n\n\n% Estes - Nike-X {Kit} (7259) [2017-]\n\n\n% Attributes\n\n\n% Decals:       Waterslide\n\n\n% Diameter:     1.3300 inches\n\n\n% Length:       23.4000 inches\n\n\n% Manufacturer:     Estes \n\n\n% Model:        7259\n\n\n% Motor Size:       18 millimeters\n\n\n% Power:        Low-Power\n\n\n% Product Type:     Kit\n\n\n% Production Years:     2017-\n\n\n% Recommended Motors:       B6-4, C6-5\n\n\n% Recovery:     Parachute\n\n\n% Skill Level:      2\n\n\n% Style:        Scale\n\n\n% Tags:     Scale:Nike:Nike-X\n\n\n% Weight:       2.4000 ounces\n\n\n%% MOTOR\n\n\n\n%% Define\n\n\n\n% Hull\n\n\nHull\n.\nName\n \n=\n \nNike-X-7259\n;\n\n\nHull\n.\nDiameter\n \n=\n \n0.03302\n;\n \n% meter\n\n\nHull\n.\nWeight\n \n=\n \n0.068\n;\n \n% grams\n\n\nHull\n.\nLength\n \n=\n \n0.594\n;\n \n% meter\n\n\n\n% Motors\n\n\nMotor1\n.\nName\n \n=\n \nD6-5\n;\n\n\nMotor1\n.\nBurnTime\n \n=\n \n1.7\n;\n \n% Seconds\n\n\nMotor1\n.\nDiameter\n \n=\n \n0.024\n;\n \n% meters\n\n\nMotor1\n.\nStaticWeight\n \n=\n \n0.023\n \n+\n \n0.01\n;\n \n% kg\n\n\nMotor1\n.\nFuelWeight\n \n=\n \n0.0211\n;\n \n% kg\n\n\nMotor1\n.\nThrust\n \n=\n \n29.7\n;\n \n% Newtons\n\n\n\nMotor2\n.\nName\n \n=\n \nC12-7\n;\n\n\nMotor2\n.\nBurnTime\n \n=\n \n1.9\n;\n \n% Seconds\n\n\nMotor2\n.\nDiameter\n \n=\n \n0.018\n;\n \n% meters\n\n\nMotor2\n.\nStaticWeight\n \n=\n \n0.0132\n;\n \n% kg\n\n\nMotor2\n.\nFuelWeight\n \n=\n \n0.0108\n;\n \n% kg\n\n\nMotor2\n.\nThrust\n \n=\n \n14.1\n;\n \n% Newtons\n\n\n\nmotors\n \n=\n \n[\nMotor1\n,\n \nMotor2\n];\n\n\n\nwind_speed\n \n=\n \n[\n10\n,\n \n0\n,\n \n0\n];\n\n\nwind_variance\n \n=\n \n[\n1\n,\n \n0\n,\n \n0\n];\n\n\n\n%% Fly dat rocket\n\n\nfilename\n \n=\n \nLAUNCH.gif\n;\n\n\nh\n \n=\n \nfigure\n(\n1\n);\n\n\nclf\n;\n\n\nplts\n \n=\n \n[];\n\n\nfor\n \ni\n \n=\n \n1\n:\nlength\n(\nmotors\n)\n  \n    \nplt\n \n=\n \nplot3\n(\n1\n,\n \n1\n,\n \n1\n,\nLineWidth\n,\n \n3\n);\n\n    \nhold\n \non\n;\n\n    \nplts\n \n=\n \n[\nplts\n,\n \nplt\n];\n\n\nend\n\n\n\nsavegif\n \n=\n \ntrue\n;\n\n\n\naxis\n([\n0\n \n10\n \n0\n \n10\n \n0\n \n1400\n]);\n\n\ngrid\n \non\n;\n\n\nxlabel\n(\nDistance (m)\n)\n\n\nylabel\n(\nDistance (m)\n)\n\n\n\ndt\n \n=\n \n10e-3\n;\n\n\n\nrocket_cross_section_axial\n \n=\n \npi\n*\n(\nHull\n.\nDiameter\n/\n2\n)\n^\n2\n;\n\n\nrocket_cross_section_orthogonal\n \n=\n \nHull\n.\nLength\n*\nHull\n.\nDiameter\n;\n\n\n\nC_d\n \n=\n \n1.17\n;\n\n\nT\n \n=\n \n273\n;\n\n\nroe\n \n=\n \n101325\n/\n(\n287.058\n*\nT\n);\n\n\n\ndrag_axial_norm\n \n=\n \n0.5\n*\nroe\n.^\n2\n*\nC_d\n*\nrocket_cross_section_axial\n;\n\n\ndrag_orthogonal_norm\n \n=\n \n0.5\n*\nroe\n.^\n2\n*\nC_d\n*\nrocket_cross_section_orthogonal\n;\n\n\n\nfor\n \nend_t\n \n=\n \n0\n:\n0.02\n:\n13\n\n    \nt\n \n=\n \n0\n:\ndt\n:\nend_t\n;\n\n    \nsamples\n \n=\n \nlength\n(\nt\n);\n\n    \nt_three\n \n=\n \nt\n.\n.*\nones\n(\nsamples\n,\n \n3\n)\n.*\ndt\n;\n\n\n    \nfor\n \ni\n \n=\n \n1\n:\nlength\n(\nmotors\n)\n  \n        \nmotor\n \n=\n \nmotors\n(\ni\n);\n\n\n        \nweight_burn_function\n \n=\n \n1\n \n-\n \nt\n/\nmotor\n.\nBurnTime\n;\n\n        \nweight_burn_function\n(\nweight_burn_function\n \n \n0\n)\n \n=\n \n0\n;\n\n\n        \nweight_burn_window\n \n=\n \nones\n(\n1\n,\n \nmin\n(\nsamples\n,\n \nmotor\n.\nBurnTime\n/\ndt\n));\n\n        \nweight_burn_window\n \n=\n \n[\nweight_burn_window\n,\n \nzeros\n(\n1\n,\n \nlength\n(\nt\n)\n \n-\n \nlength\n(\nweight_burn_window\n))];\n\n\n        \n% Intialize Rocket Properties\n\n        \nacceleration\n  \n=\n \n[\n0\n,\n \n0\n,\n \n-\n9.8\n]\n.*\nones\n(\nsamples\n,\n \n3\n);\n\n        \nvelocity\n      \n=\n \n[\n0\n,\n \n0\n,\n \n0\n]\n.*\nones\n(\nsamples\n,\n \n3\n);\n\n        \nposition\n      \n=\n \n[\n0\n,\n \n0\n,\n \n0\n]\n.*\nones\n(\nsamples\n,\n \n3\n);\n\n        \nattack_vector\n \n=\n \n[\n0.001\n,\n \n0.001\n,\n \n1\n]\n.*\nones\n(\nsamples\n,\n \n3\n);\n \n% [Rotation, Elevation]\n\n        \nattack_vector\n \n=\n \nattack_vector\n./\nsum\n(\nattack_vector\n(\n1\n,\n \n:));\n\n\n        \ntotal_static_mass\n \n=\n \nHull\n.\nWeight\n \n+\n \nmotor\n.\nStaticWeight\n;\n\n        \nengine_acceleration\n \n=\n \nmotor\n.\nThrust\n.*\nweight_burn_window\n./\n(\ntotal_static_mass\n \n+\n \nmotor\n.\nFuelWeight\n.*\nweight_burn_function\n);\n\n\n        \nacceleration\n \n=\n \nacceleration\n \n+\n \nattack_vector\n.*\nengine_acceleration\n.\n;\n\n        \nvelocity\n \n=\n \ncumsum\n(\nacceleration\n.*\nt_three\n)\n \n+\n \nvelocity\n;\n\n        \nposition\n \n=\n \ncumsum\n(\nvelocity\n.*\nt_three\n.^\n2\n)\n \n+\n \nposition\n;\n\n\n        \nfor\n \nj\n \n=\n \n2\n:\nsamples\n\n            \nif\n(\nposition\n(\nj\n,\n \n3\n)\n \n \n0\n)\n\n                \nposition\n(\nj\n,\n \n3\n)\n \n=\n \n0\n;\n\n                \nposition\n(\nj\n,\n \n2\n)\n \n=\n \nposition\n(\nj\n-\n1\n,\n \n2\n);\n\n                \nposition\n(\nj\n,\n \n1\n)\n \n=\n \nposition\n(\nj\n-\n1\n,\n \n1\n);\n\n            \nend\n\n        \nend\n\n\n        \nplts\n(\ni\n).\nXData\n \n=\n \nposition\n(:,\n \n1\n)\n*\n3.28084\n;\n\n        \nplts\n(\ni\n).\nYData\n \n=\n \nposition\n(:,\n \n2\n)\n*\n3.28084\n;\n\n        \nplts\n(\ni\n).\nZData\n \n=\n \nposition\n(:,\n \n3\n)\n*\n3.28084\n;\n\n    \nend\n\n    \nview\n(\n45\n*\n(\nend_t\n/\n13\n \n-\n \n0.5\n),\n30\n)\n\n\n    \ndrawnow\n;\n\n\n    \nif\n \nsavegif\n\n        \n% Write to the GIF File \n\n        \nif\n \nend_t\n \n==\n \n0\n \n            \ngif\n(\nfilename\n,\nDelayTime\n,\n0.02\n,\nframe\n,\nh\n);\n\n        \nelse\n \n            \ngif\n;\n\n        \nend\n \n    \nend\n\n\nend\n\n\n\n\n\n\nReferences\n\n\n[1] VMS, MIT OpenCourseWare \nVMS Lecture\n.\n\n\n[2] GIF saving tool, Mathworks file exchange \ngif\n.", 
            "title": "ROCKETS"
        }, 
        {
            "location": "/rockets/#rockets", 
            "text": "", 
            "title": "ROCKETS"
        }, 
        {
            "location": "/rockets/#background", 
            "text": "For the last few years, I've attended a summer BBQ event hosted by the company my girlfriend works for in eastern Pennsylvania. My personal favorite part of this event is the one involving controlled explosions (of course) in which everyone participates in assembling a rocket to fire off. The rockets range from the toy-sized, to the massive almost 6' in length.   This year, I was given A very simple choice of which off the shelf rocket and motor to use. Do I use a  D6  engine or a  C12  engine with my  Nike-X-7259 ? I mean the choice was simple but not having much experience with hobby rockets and of course when you spend the many of your days modeling systems for engineering purposes, the answer can't be  that  easy...  So I decided to start working on a very simple model of rocket dynamics to try to determine which of these engines will give the best performance. I needed a way to validate my intuition which told me that the larger motor crammed into the smaller rocket body (which it was not setup for) would be the way to go. The result of this pondering is the beginnings of a modeling tool in MATLAB to help me determine the overall trajectory for a given configuration.   The simulation takes into account: Hull Weight, Hull Dimensions, Motor Thrust, Motor Burn Times, Motor Static Weight, Motor Fuel Weight, Motor Thrust. I'm beginning to work on integrating drag and other non-idealities which will probably happen sometime before next years event. Below is a simple comparison of the two configurations. I adjusted the displacement angle slightly off normal so that we can actually see how they might behave more easily.    See MATLAB section below for source  Well that convinces me enough to go with the D6, showing that even with the higher static weight and shorter burn time, the D6 should provide a higher maximum altitude reached. The only other tricky bit was ensuring that the rocket maintained aerodynamic stability with that added weight in the back (since this rocket was designed to use the C12). To account for how that added clay might impact the results, I added 0.01 kg to the static weight in the Motor1 scenario.   After a custom engine fitting and excellent paint job by my talented other half, we have a rocket! (Mine is the far left)   On at launch time it had already gotten pretty dark so I've pointed out which one is mine in the image below. It's also the last to leave the platform... So heavy...   Anyways, here is the launch. Unfortunately because it was so dark we lost track of it pretty much immediately, but it was found in the end. I'm hoping to eventually compare the launch in greater detail with the simulation but first must add at least some very basic wind effects, since a parachute deploys on descent. Another feature for next time!", 
            "title": "Background"
        }, 
        {
            "location": "/rockets/#matlab", 
            "text": "% Estes - Nike-X {Kit} (7259) [2017-]  % Attributes  % Decals:       Waterslide  % Diameter:     1.3300 inches  % Length:       23.4000 inches  % Manufacturer:     Estes   % Model:        7259  % Motor Size:       18 millimeters  % Power:        Low-Power  % Product Type:     Kit  % Production Years:     2017-  % Recommended Motors:       B6-4, C6-5  % Recovery:     Parachute  % Skill Level:      2  % Style:        Scale  % Tags:     Scale:Nike:Nike-X  % Weight:       2.4000 ounces  %% MOTOR  %% Define  % Hull  Hull . Name   =   Nike-X-7259 ;  Hull . Diameter   =   0.03302 ;   % meter  Hull . Weight   =   0.068 ;   % grams  Hull . Length   =   0.594 ;   % meter  % Motors  Motor1 . Name   =   D6-5 ;  Motor1 . BurnTime   =   1.7 ;   % Seconds  Motor1 . Diameter   =   0.024 ;   % meters  Motor1 . StaticWeight   =   0.023   +   0.01 ;   % kg  Motor1 . FuelWeight   =   0.0211 ;   % kg  Motor1 . Thrust   =   29.7 ;   % Newtons  Motor2 . Name   =   C12-7 ;  Motor2 . BurnTime   =   1.9 ;   % Seconds  Motor2 . Diameter   =   0.018 ;   % meters  Motor2 . StaticWeight   =   0.0132 ;   % kg  Motor2 . FuelWeight   =   0.0108 ;   % kg  Motor2 . Thrust   =   14.1 ;   % Newtons  motors   =   [ Motor1 ,   Motor2 ];  wind_speed   =   [ 10 ,   0 ,   0 ];  wind_variance   =   [ 1 ,   0 ,   0 ];  %% Fly dat rocket  filename   =   LAUNCH.gif ;  h   =   figure ( 1 );  clf ;  plts   =   [];  for   i   =   1 : length ( motors )   \n     plt   =   plot3 ( 1 ,   1 ,   1 , LineWidth ,   3 ); \n     hold   on ; \n     plts   =   [ plts ,   plt ];  end  savegif   =   true ;  axis ([ 0   10   0   10   0   1400 ]);  grid   on ;  xlabel ( Distance (m) )  ylabel ( Distance (m) )  dt   =   10e-3 ;  rocket_cross_section_axial   =   pi * ( Hull . Diameter / 2 ) ^ 2 ;  rocket_cross_section_orthogonal   =   Hull . Length * Hull . Diameter ;  C_d   =   1.17 ;  T   =   273 ;  roe   =   101325 / ( 287.058 * T );  drag_axial_norm   =   0.5 * roe .^ 2 * C_d * rocket_cross_section_axial ;  drag_orthogonal_norm   =   0.5 * roe .^ 2 * C_d * rocket_cross_section_orthogonal ;  for   end_t   =   0 : 0.02 : 13 \n     t   =   0 : dt : end_t ; \n     samples   =   length ( t ); \n     t_three   =   t . .* ones ( samples ,   3 ) .* dt ; \n\n     for   i   =   1 : length ( motors )   \n         motor   =   motors ( i ); \n\n         weight_burn_function   =   1   -   t / motor . BurnTime ; \n         weight_burn_function ( weight_burn_function     0 )   =   0 ; \n\n         weight_burn_window   =   ones ( 1 ,   min ( samples ,   motor . BurnTime / dt )); \n         weight_burn_window   =   [ weight_burn_window ,   zeros ( 1 ,   length ( t )   -   length ( weight_burn_window ))]; \n\n         % Intialize Rocket Properties \n         acceleration    =   [ 0 ,   0 ,   - 9.8 ] .* ones ( samples ,   3 ); \n         velocity        =   [ 0 ,   0 ,   0 ] .* ones ( samples ,   3 ); \n         position        =   [ 0 ,   0 ,   0 ] .* ones ( samples ,   3 ); \n         attack_vector   =   [ 0.001 ,   0.001 ,   1 ] .* ones ( samples ,   3 );   % [Rotation, Elevation] \n         attack_vector   =   attack_vector ./ sum ( attack_vector ( 1 ,   :)); \n\n         total_static_mass   =   Hull . Weight   +   motor . StaticWeight ; \n         engine_acceleration   =   motor . Thrust .* weight_burn_window ./ ( total_static_mass   +   motor . FuelWeight .* weight_burn_function ); \n\n         acceleration   =   acceleration   +   attack_vector .* engine_acceleration . ; \n         velocity   =   cumsum ( acceleration .* t_three )   +   velocity ; \n         position   =   cumsum ( velocity .* t_three .^ 2 )   +   position ; \n\n         for   j   =   2 : samples \n             if ( position ( j ,   3 )     0 ) \n                 position ( j ,   3 )   =   0 ; \n                 position ( j ,   2 )   =   position ( j - 1 ,   2 ); \n                 position ( j ,   1 )   =   position ( j - 1 ,   1 ); \n             end \n         end \n\n         plts ( i ). XData   =   position (:,   1 ) * 3.28084 ; \n         plts ( i ). YData   =   position (:,   2 ) * 3.28084 ; \n         plts ( i ). ZData   =   position (:,   3 ) * 3.28084 ; \n     end \n     view ( 45 * ( end_t / 13   -   0.5 ), 30 ) \n\n     drawnow ; \n\n     if   savegif \n         % Write to the GIF File  \n         if   end_t   ==   0  \n             gif ( filename , DelayTime , 0.02 , frame , h ); \n         else  \n             gif ; \n         end  \n     end  end", 
            "title": "MATLAB"
        }, 
        {
            "location": "/rockets/#references", 
            "text": "[1] VMS, MIT OpenCourseWare  VMS Lecture .  [2] GIF saving tool, Mathworks file exchange  gif .", 
            "title": "References"
        }, 
        {
            "location": "/about/", 
            "text": "About Me", 
            "title": "About Me"
        }, 
        {
            "location": "/about/#about-me", 
            "text": "", 
            "title": "About Me"
        }
    ]
}